{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOclY7o3csOxVRbz+jYfIXH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joyuno/lguplus_project/blob/main/%EC%BB%A4%EB%AE%A4%EB%8B%88%ED%8B%B0_%EC%A1%B0%EC%82%AC_%EC%99%84%EC%84%B1%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6e_2wUs68YQo"
      },
      "outputs": [],
      "source": [
        "!pip install beautifulsoup4\n",
        "!pip install requests\n",
        "!pip install selenium\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zrtCZBfbhpi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        
        "def selenium_scroll_option():\n",
        "  SCROLL_PAUSE_SEC = 3\n",
        "\n",
       
        "  last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "\n",
        "  while True:\n",
       
        "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "\n",
        
        "    time.sleep(SCROLL_PAUSE_SEC)\n",
        "\n",
        
        "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "#revolution_main_table > tbody > tr:nth-child(3) > td:nth-child(2) > a > span\n",
        "#revolution_main_table > tbody > tr:nth-child(4) > td:nth-child(2) > a > span\n",
        "\n",
        "    if new_height == last_height:\n",
        "        break\n",
        "    last_height = new_height"
      ],
      "metadata": {
        "id": "pqVxx1_rwV5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import selenium\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.common.by import By\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import time\n",
        "import re\n",
        "import sys\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        
        "driver = webdriver.Chrome(options=chrome_options)\n",
        "# 키워드 목록\n",
        "keywords = [\"이야기\",\"마블\",\"헬모\",\"아이즈\",\"유모바일\",\"모빙\",\"티플러스\",\"알닷\",\"에르엘\",\"프리티\",\"슈가\",\"Sugar\",\"sugar\",\n",
        "            \"에이모바일\",\"kg\",\"에이모\", \"찬스\",\"핀다이렉트\", \"이지모바일\",\"스노우맨\", \"스노우\",\"보스\",\"프리텔\",\"와이엘\",\"와이엘랜드\",\n",
        "            \"YL\",\"A모바일\", \"벨류컴\",\"인스모바일\",\"인스\",\"리브엠\",\"리브모바일\",\"모나\",\"토스모바일\",\"토스\",\"유플\",\"코나아이\",\"EG\",\n",
        "            \"에넥스\",\"헬로\",\"스마텔\",\"위너스텔\",\"친구모바일\",\"에넥스\",\"유니컴즈\",\"KPMO\",\"KB\",\"에스원\",\"엔티온\",\"LG\",\"lg\",\n",
        "            \"헬로비전\",\"핀샷\",\"화인\",\"CK\",\"ck\",\"에이프러스\",\"슈거\",\"유모알\",\"유모비\",\"liv\",\"핀다\",\"시월\"]\n",
        "\n",
        "# 크롤링할 페이지 범위\n",
        "start_page = 1\n",
        "end_page = 2\n",
        "\n",
        "# 두 글자 이상 키워드 패턴 생성\n",
        "keyword_pattern = re.compile('|'.join(re.escape(keyword) for keyword in keywords if len(keyword) >= 2))\n",
        "\n",
        "base_url = 'https://www.ppomppu.co.kr/zboard/zboard.php?id=phone&page={page_num}&category=6&divpage=691'\n",
        "title = []\n",
        "titles = []\n",
        "paragraphs = []\n",
        "for page_num in range(start_page, end_page + 1):\n",
        "    url = base_url.format(page_num=page_num)\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        
        "    table = soup.find(id=\"revolution_main_table\")\n",
        "    rows = table.find_all(\"tr\")\n",
        "\n",
        "    for row in rows:\n",
        "        cells = row.find_all(\"td\", class_=\"baseList-space\")\n",
        "\n",
        "        for cell in cells:\n",
        "            a_tag = cell.find(\"a\", class_=\"baseList-title\")\n",
        "\n",
        "            if a_tag:\n",
        "                spans = a_tag.find_all(\"span\")  # select 대신 find_all 사용하여 모든 span 요소 가져옴\n",
        "                for span in spans:\n",
        "                    span_text = span.get_text().strip()\n",

        "                    # 두 글자 이상 일치하는 키워드가 있는 경우\n",
        "                    if keyword_pattern.search(span_text):\n",
        "                        #title = [text for text in span_text if keyword_pattern.search(text)]\n",
        "                        title.append(span_text)\n",

        "                        href = a_tag['href']\n",
        "                        full_link = \"https://www.ppomppu.co.kr/zboard/\" + href\n",
        "\n",

        "                        detail_response = requests.get(full_link)\n",
        "                        print(full_link)\n",
        "                        detail_soup = BeautifulSoup(detail_response.text, 'html.parser')\n",

        "                        han_text = detail_soup.select_one('.han')\n",
        "                        if han_text:  # .han 요소가 존재할 때만 처리\n",
        "                            final_text = han_text.get_text().strip()\n",
        "                            paragraphs.append(final_text.replace('\\xa0', '').replace('\\n', ''))\n",
        "                            titles.append(title)\n",
        "                        #for paragraph in paragraphs:\n",
        "data={'title':title,'paragraph':paragraphs}\n",
        "print(data)\n",
        "\n",
        "  #revolution_main_table > tbody > tr:nth-child(7) > td:nth-child(2) > a > span\n",
        "#https://gall.dcinside.com/mgallery/board/lists/?id=mvnogallery&page=5"
      ],
      "metadata": {
        "id": "Pp3i2gSTdm68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "RsR-gGgnKtIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len=[len(value) for key, value in data.items()]\n",
        "len"
      ],
      "metadata": {
        "id": "8A7wY8aSH0FH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "uuTNzIbiHvjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"searle-j/kote_for_easygoing_people\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"searle-j/kote_for_easygoing_people\")"
      ],
      "metadata": {
        "id": "qjdl92TIH2Sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.config.id2label) #44개의 라벨링\n"
      ],
      "metadata": {
        "id": "qYlwfhecXLou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# 감정분석 함수 정의 (텍스트 레이블로 변환)\n",
        "def analyze_sentiment_with_score(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    probs = torch.softmax(logits, dim=1)\n",
        "    max_prob, predicted_class = torch.max(probs, dim=1)\n",
        "    predicted_class = predicted_class.item()\n",
        "    max_prob = max_prob.item()\n",
        "\n",
        "    # 모델 설정에 있는 id2label을 사용하여 숫자를 텍스트 레이블로 변환\n",
        "    # id2label은 key가 int 또는 str일 수 있으므로 둘 다 확인\n",
        "    if predicted_class in model.config.id2label:\n",
        "        label = model.config.id2label[predicted_class]\n",
        "    elif str(predicted_class) in model.config.id2label:\n",
        "        label = model.config.id2label[str(predicted_class)]\n",
        "    else:\n",
        "        label = str(predicted_class)  # 매핑이 없는 경우 숫자를 그대로 반환\n",
        "\n",
        "    return label, max_prob\n",
        "\n",
        "# \"title\"과 \"paragraph\"를 결합하여 분석할 텍스트 생성\n",
        "df[\"combined_text\"] = df[\"title\"] + \" \" + df[\"paragraph\"]\n",
        "\n",
        "# 각 행에 대해 감정분석 수행하여 텍스트 레이블 결과 컬럼 추가\n",
        "df[\"sentiment_label\"], df[\"sentiment_score\"] = zip(*df[\"combined_text\"].apply(analyze_sentiment_with_score))\n",
        "\n",
        "# 결과 출력\n",
        "print(df[[\"title\", \"sentiment_label\"]])"
      ],
      "metadata": {
        "id": "FHOlewGYJvxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "df[[\"combined_text\", \"sentiment_label\",'sentiment_score']].head()"
      ],
      "metadata": {
        "id": "Hu5My5GjKrk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "highest_comments = df.loc[df.groupby(\"sentiment_label\")[\"sentiment_score\"].idxmax()]\n",
        "pd.reset_option('display.max_colwidth', None)\n"
      ],
      "metadata": {
        "id": "ArwJXjRCNoOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " df['sentiment_label'].value_counts()"
      ],
      "metadata": {
        "id": "WxNvrrZqObBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 결과 출력 (라벨, combined_text, sentiment_score 확인)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "highest_comments[[\"sentiment_label\", \"combined_text\", \"sentiment_score\"]]"
      ],
      "metadata": {
        "id": "AYswjQC1OVAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TT0qss2fRjOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n,m = list(map(int,input().split()))\n",
        "\n",
        "s = []\n",
        "\n",
        "def dfs():\n",
        "    if len(s)==m:\n",
        "        print(' '.join(map(str,s)))\n",
        "        return\n",
        "\n",
        "    for i in range(1,n+1):\n",
        "        if i not in s:\n",
        "            s.append(i)\n",
        "            dfs()\n",
        "            s.pop()\n",
        "\n",
        "dfs()"
      ],
      "metadata": {
        "id": "h1KTTZlFRi5A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
